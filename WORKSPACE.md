# JSONPlaceholder MCP Server + Streamlit Agent

A production-ready monorepo containing:

1. **MCP Server** (`app/`) â€” Wraps JSONPlaceholder API with structured tools via Model Context Protocol
2. **Streamlit Agent** (`streamlit_agent/`) â€” AI-powered chat interface with tool calling capabilities

Both components work together but can run and deploy independently.

## Quick Start

### Setup Environment

```bash
# Clone and navigate to project
cd sample-mcp-server

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### Running MCP Server

```bash
# Install dependencies
pip install -r requirements.txt

# Start server (Terminal 1)
python -m app.main
# Output: Initializing JSONPlaceholder MCP Server on 0.0.0.0:8000
```

### Running Streamlit Agent

```bash
# Install dependencies (different from MCP server)
pip install -r streamlit_agent/requirements.txt

# Configure environment
cp streamlit_agent/.env.example streamlit_agent/.env
# Edit .env with your LLM API keys and MCP server URL

# Start agent (Terminal 2)
streamlit run streamlit_agent/app.py
# Opens http://localhost:8501
```

## Project Structure

```
sample-mcp-server/
â”‚
â”œâ”€â”€ app/                                 # MCP Server
â”‚   â”œâ”€â”€ main.py                          # Entry point
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ mcp_server.py                # MCP server implementation
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ jsonplaceholder_client.py # HTTP client for JSONPlaceholder
â”‚       â””â”€â”€ tools/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ posts.py                 # Post-related MCP tools
â”‚           â””â”€â”€ users.py                 # User-related MCP tools
â”‚
â”œâ”€â”€ streamlit_agent/                     # Streamlit Agent
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                           # Main Streamlit app
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent_logic.py               # Agent orchestration
â”‚   â”‚   â”œâ”€â”€ llm_client.py                # LLM provider abstraction
â”‚   â”‚   â””â”€â”€ mcp_client.py                # MCP server client
â”‚   â”œâ”€â”€ requirements.txt                 # Streamlit-specific deps
â”‚   â”œâ”€â”€ .env.example                     # Configuration template
â”‚   â”œâ”€â”€ README.md                        # Streamlit agent docs
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ requirements.txt                     # MCP server deps
â”œâ”€â”€ .env.example                         # MCP server config template
â”œâ”€â”€ Procfile                             # Render deployment config
â”œâ”€â”€ README.md                            # This file
â””â”€â”€ .gitignore
```

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User (Browser)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Streamlit App      â”‚
                    â”‚  (http://8501)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM Provider   â”‚  â”‚ Session     â”‚  â”‚ MCP Client   â”‚
    â”‚ (OpenAI/Groq)  â”‚  â”‚ State       â”‚  â”‚ (HTTP)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  MCP Server                â”‚
                              â”‚  (http://8000)            â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ JSONPlaceholder API        â”‚
                              â”‚ (https://...)              â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## MCP Server (`app/`)

### What It Does

Exposes **5 structured MCP tools** that wrap JSONPlaceholder API:
- `get_post(post_id)` â€” Fetch single post
- `list_posts(user_id=None)` â€” List posts, optionally filtered
- `get_comments_for_post(post_id)` â€” Get post comments
- `get_user(user_id)` â€” Get user details
- `list_users()` â€” List all users

### Features

âœ… **Production-Ready**
- Type hints throughout
- Comprehensive error handling
- Request timeouts (10s)
- Structured JSON responses
- Full logging

âœ… **Clean Architecture**
- Service layer for HTTP calls
- Tools layer for MCP definitions
- Server layer for orchestration
- Main entry point for startup

âœ… **Deployment Ready**
- Environment variable configuration
- 0.0.0.0 binding for containers
- Render Procfile included
- Health check endpoint

### Configuration

**Environment Variables:**
```bash
HOST=0.0.0.0          # Bind address
PORT=8000             # Listen port
```

### Running

```bash
# Local
python -m app.main

# With custom port
PORT=3000 python -m app.main

# Check health
curl http://localhost:8000/health
```

### Documentation

See [app/README.md](app/../README.md) for detailed documentation.

## Streamlit Agent (`streamlit_agent/`)

### What It Does

Interactive chat UI that:
1. Takes user messages
2. Sends to LLM (OpenAI, Groq, Hugging Face)
3. LLM detects when tools are needed
4. Agent calls tools via MCP server
5. Returns aggregated results to LLM
6. Streams final response to user

### Features

âœ… **Multi-Provider LLM Support**
- OpenAI (GPT-4, GPT-3.5)
- Groq (Mixtral, Llama2)
- Hugging Face Inference API

âœ… **Rich UI**
- Real-time streaming responses
- Sidebar configuration panel
- Tool call indicators
- MCP health check
- Conversation memory
- Custom system prompt

âœ… **Production Architecture**
- Type-safe agent logic
- Graceful error handling
- Session state management
- Comprehensive logging

### Configuration

**Environment Variables:**
```bash
LLM_PROVIDER=openai              # openai, groq, huggingface
LLM_API_KEY=sk-...               # Your API key
LLM_MODEL=gpt-4o-mini            # Model name
MCP_SERVER_URL=http://localhost:8000  # MCP server address
```

### Running

```bash
# Install dependencies
pip install -r streamlit_agent/requirements.txt

# Create .env file
cp streamlit_agent/.env.example streamlit_agent/.env
# Edit with your API keys

# Start app
streamlit run streamlit_agent/app.py
```

Opens at `http://localhost:8501`

### Usage Examples

```
User: "Show me posts by user 5"
Agent: Calls list_posts(user_id=5) â†’ Displays results

User: "Get comments for post 1"
Agent: Calls get_comments_for_post(post_id=1) â†’ Shows comments

User: "Tell me about user 3"
Agent: Calls get_user(user_id=3) â†’ Describes user

User: "How many posts does user 2 have?"
Agent: Calls list_posts(user_id=2) â†’ Counts and responds
```

### Documentation

See [streamlit_agent/README.md](streamlit_agent/README.md) for detailed documentation.

## Deployment

### MCP Server

**Render:**
```bash
# Build Command
pip install -r requirements.txt

# Start Command
web: python -m app.main
```

**Environment Variables:**
```
PORT=8000
HOST=0.0.0.0
```

### Streamlit Agent

**Streamlit Cloud:**
1. Push to GitHub
2. Create app from `streamlit_agent/app.py`
3. Add secrets: `LLM_API_KEY`, `MCP_SERVER_URL`

**Render:**
```bash
# Build Command
pip install -r streamlit_agent/requirements.txt

# Start Command
web: streamlit run streamlit_agent/app.py
```

**Railway:**
Deploy both as separate services:
- Service 1: MCP Server on port 8000
- Service 2: Streamlit on port 8501

## Development

### Running Both Locally

**Terminal 1 - MCP Server:**
```bash
python -m app.main
```

**Terminal 2 - Streamlit Agent:**
```bash
streamlit run streamlit_agent/app.py
```

### Testing

**Test MCP Server:**
```bash
python << 'EOF'
from app.server.services import JSONPlaceholderClient

client = JSONPlaceholderClient()
post = client.get_post(1)
print(post)
EOF
```

**Test LLM Client:**
```bash
python << 'EOF'
from streamlit_agent.agent import LLMClient

llm = LLMClient.create("openai", "sk-...", "gpt-4o-mini")
response = llm.send_message([{"role": "user", "content": "Hello"}])
print(response)
EOF
```

### Debugging

Enable debug logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Code Quality

Both projects include:
- âœ… **Type Hints** â€” Full type annotations
- âœ… **Docstrings** â€” Comprehensive documentation
- âœ… **Logging** â€” Detailed operation tracking
- âœ… **Error Handling** â€” Graceful failure modes
- âœ… **Clean Code** â€” PEP 8 compliant

## Troubleshooting

### MCP Server won't start
```bash
# Check port is available
lsof -i :8000

# Verify Python version
python --version  # 3.8+

# Check dependencies
pip list | grep mcp
```

### Streamlit app can't reach MCP server
```bash
# Verify MCP server is running
curl http://localhost:8000/health

# Check MCP_SERVER_URL in .env
cat streamlit_agent/.env | grep MCP_SERVER_URL

# Test LLM API key
python -c "from openai import OpenAI; OpenAI(api_key='YOUR_KEY')"
```

### LLM API key errors
```bash
# Ensure .env is in correct location
ls streamlit_agent/.env

# Check environment variable is loaded
python -c "import os; os.environ.get('LLM_API_KEY')"

# Verify no extra quotes/spaces in .env
cat streamlit_agent/.env | grep LLM_API_KEY
```

## Performance Notes

- **MCP Server**: Handles ~100 req/s with proper async patterns
- **Streamlit Agent**: Streams responses for better UX
- **Timeouts**: 10s on MCP calls, 30s on LLM calls
- **Session State**: In-memory conversation history

## Security

- âœ… **No hardcoded credentials** â€” All secrets via environment variables
- âœ… **Input validation** â€” All tool arguments validated
- âœ… **Type safety** â€” Type hints prevent injection
- âœ… **HTTPS ready** â€” Can be deployed behind reverse proxy

## License

MIT License

## Support

For issues:
1. Check service-specific README files
2. Verify configuration with `echo $VARIABLE_NAME`
3. Check logs in terminal running services
4. Test connectivity: `curl http://localhost:8000/health`

---

**Happy building! ğŸš€**
